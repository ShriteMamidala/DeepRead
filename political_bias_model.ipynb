{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf49b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted 1643 labeled phrases and saved to basil_phrase_level_binary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "root_dir = r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\DeepRead\\DeepRead\\data\\BASIL\\annotations\"\n",
    "data = []\n",
    "\n",
    "# Loop through all year folders (2010-2019)\n",
    "for folder in os.listdir(root_dir):\n",
    "    folder_path = os.path.join(root_dir, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            json_path = os.path.join(folder_path, filename)\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                article = json.load(f)\n",
    "\n",
    "                # Process phrase-level annotations\n",
    "                for phrase in article.get(\"phrase-level-annotations\", []):\n",
    "                    text = phrase.get(\"txt\", \"\").strip()\n",
    "                    bias = phrase.get(\"bias\", \"\").lower()\n",
    "                    \n",
    "                    # Binary label: 1 = biased, 0 = neutral\n",
    "                    label = 0 if bias in [\"\", \"none\"] else 1\n",
    "\n",
    "                    if text:\n",
    "                        data.append({\"text\": text, \"label\": label})\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame(data)\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv(\"basil_phrase_level_binary.csv\", index=False)\n",
    "print(f\"âœ… Extracted {len(df)} labeled phrases and saved to basil_phrase_level_binary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515dee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 1589 unbiased lines to basil_neutral_lines_1600.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "article_dir = r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\DeepRead\\DeepRead\\data\\BASIL\\articles\"\n",
    "annotation_dir = r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\DeepRead\\DeepRead\\data\\BASIL\\annotations\"\n",
    "\n",
    "neutral_data = []\n",
    "max_neutral = 1600\n",
    "\n",
    "# Walk through year folders\n",
    "for year in os.listdir(article_dir):\n",
    "    year_article_path = os.path.join(article_dir, year)\n",
    "    year_annotation_path = os.path.join(annotation_dir, year)\n",
    "\n",
    "    if not os.path.isdir(year_article_path):\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(year_article_path):\n",
    "        if not file.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        article_path = os.path.join(year_article_path, file)\n",
    "        annotation_path = os.path.join(year_annotation_path, file)\n",
    "\n",
    "        # Load article text\n",
    "        with open(article_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            article = json.load(f)\n",
    "\n",
    "        # Flatten lines from all paragraphs\n",
    "        lines = [line.strip() for para in article.get(\"body-paragraphs\", []) for line in para if line.strip()]\n",
    "\n",
    "        # Load annotation and collect all biased lines\n",
    "        annotated_lines = set()\n",
    "        if os.path.exists(annotation_path):\n",
    "            with open(annotation_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                annotations = json.load(f)\n",
    "                annotated_lines = set(p.get(\"txt\", \"\").strip() for p in annotations.get(\"phrase-level-annotations\", []))\n",
    "\n",
    "        # Add non-annotated lines\n",
    "        for line in lines:\n",
    "            if len(neutral_data) >= max_neutral:\n",
    "                break\n",
    "            if line not in annotated_lines:\n",
    "                neutral_data.append({\"text\": line, \"label\": 0})\n",
    "\n",
    "        if len(neutral_data) >= max_neutral:\n",
    "            break\n",
    "\n",
    "# Save to CSV\n",
    "df_neutral = pd.DataFrame(neutral_data).drop_duplicates().head(max_neutral)\n",
    "df_neutral.to_csv(\"basil_neutral_lines_1600.csv\", index=False)\n",
    "print(f\"âœ… Saved {len(df_neutral)} unbiased lines to basil_neutral_lines_1600.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba7b10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined dataset saved to:\n",
      "C:\\Users\\shrit\\Desktop\\Ml_Projects\\DeepRead\\DeepRead\\basil_combined_dataset.csv\n",
      "Total rows: 3232 | Biased: 1643 | Neutral: 1589\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to your files\n",
    "biased_path = r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\DeepRead\\DeepRead\\basil_phrase_level_binary.csv\"\n",
    "neutral_path = r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\DeepRead\\DeepRead\\basil_neutral_lines_1600.csv\"\n",
    "\n",
    "# Load both datasets\n",
    "df_biased = pd.read_csv(biased_path)\n",
    "df_neutral = pd.read_csv(neutral_path)\n",
    "\n",
    "# Optional: keep columns consistent\n",
    "df_biased = df_biased[['text', 'label']]\n",
    "df_neutral = df_neutral[['text', 'label']]\n",
    "\n",
    "# Combine and shuffle\n",
    "df_combined = pd.concat([df_biased, df_neutral], ignore_index=True).drop_duplicates()\n",
    "df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save combined dataset\n",
    "output_path = r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\DeepRead\\DeepRead\\basil_combined_dataset.csv\"\n",
    "df_combined.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Combined dataset saved to:\\n{output_path}\")\n",
    "print(f\"Total rows: {len(df_combined)} | Biased: {df_combined['label'].sum()} | Neutral: {(df_combined['label'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc0a7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129d263b439c44ac8d55a75a6644968e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2585 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0e56239bca4c7897a0103c99e1bd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\shrit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\shrit\\AppData\\Local\\Temp\\ipykernel_12000\\2263245304.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='405' max='405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [405/405 03:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.356366</td>\n",
       "      <td>0.834621</td>\n",
       "      <td>0.813264</td>\n",
       "      <td>0.906615</td>\n",
       "      <td>0.737342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.342302</td>\n",
       "      <td>0.850077</td>\n",
       "      <td>0.831304</td>\n",
       "      <td>0.922780</td>\n",
       "      <td>0.756329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.354399</td>\n",
       "      <td>0.854714</td>\n",
       "      <td>0.840136</td>\n",
       "      <td>0.908088</td>\n",
       "      <td>0.781646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.363430</td>\n",
       "      <td>0.853168</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.857605</td>\n",
       "      <td>0.838608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.371555</td>\n",
       "      <td>0.853168</td>\n",
       "      <td>0.842454</td>\n",
       "      <td>0.885017</td>\n",
       "      <td>0.803797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.34230175614356995, 'eval_accuracy': 0.8500772797527048, 'eval_f1': 0.831304347826087, 'eval_precision': 0.9227799227799228, 'eval_recall': 0.7563291139240507, 'eval_runtime': 2.2183, 'eval_samples_per_second': 291.662, 'eval_steps_per_second': 9.467, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\shrit\\Desktop\\Ml_Projects\\DeepRead\\DeepRead\\data\\BASIL\\political_final_data.csv\")\n",
    "\n",
    "# Drop any rows with missing values\n",
    "data = data.dropna(subset=[\"text\", \"label\"])\n",
    "\n",
    "# Ensure labels are int\n",
    "data[\"label\"] = data[\"label\"].astype(int)\n",
    "\n",
    "# Convert to Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Train/test split\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = split_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load DistilBERT for binary classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define metrics (accuracy + precision/recall/f1)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_bias_binary\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16682d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./political_bias_tokenizer\\\\tokenizer_config.json',\n",
       " './political_bias_tokenizer\\\\special_tokens_map.json',\n",
       " './political_bias_tokenizer\\\\vocab.txt',\n",
       " './political_bias_tokenizer\\\\added_tokens.json',\n",
       " './political_bias_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./political_bias_model\")\n",
    "tokenizer.save_pretrained(\"./political_bias_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cb579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
